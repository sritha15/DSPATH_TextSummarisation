{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5X_m2xL3HE8",
        "outputId": "b3b62ac6-5eca-43a0-a439-e750ccc6f104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.247)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.5.13)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.15)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi40Zd1i3MGJ",
        "outputId": "13112018-b14b-4941-95a0-336c92dd80bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"  # replace with your actual API key\n"
      ],
      "metadata": {
        "id": "Qc4H76FW3TSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "response = openai.Completion.create(\n",
        "  engine=\"text-davinci-002\",\n",
        "  prompt=\"Translate the following English text to French: '{hi}'\",\n",
        "  max_tokens=60\n",
        ")\n",
        "\n",
        "print(response.choices[0].text.strip())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hf0c7Ea692H",
        "outputId": "754e307f-cb0c-440d-e8c4-0403973f4873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "\n",
        "template = \"Hi what is your {adjective}?\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "prompt.format(adjective=\"age\")\n"
      ],
      "metadata": {
        "id": "Xu_8OD5V7wRJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "outputId": "64eac120-d63e-4794-a31f-2b6649369608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi what is your age?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# An example prompt with no input variables\n",
        "no_input_prompt = PromptTemplate(input_variables=[], template=\"Tell me a joke.\")\n",
        "no_input_prompt.format()\n",
        "# -> \"Tell me a joke.\"\n",
        "\n",
        "\n",
        "# An example prompt with multiple input variables\n",
        "\n",
        "# -> \"Tell me a funny joke about chickens.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "8NQviEO4Py37",
        "outputId": "4b081c2e-4e49-4544-f0d5-b40bcab0a3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me a joke.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# An example prompt with one input variable\n",
        "one_input_prompt = PromptTemplate(input_variables=[\"adjective\"], template=\"Tell me a {adjective} joke.\")\n",
        "one_input_prompt.format(adjective=\"funny\")\n",
        "# -> \"Tell me a funny joke.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "SYIjwhVHW9T-",
        "outputId": "31572005-4ac6-4992-d799-9c07cc77f5a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me a funny joke.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multiple_input_prompt = PromptTemplate(\n",
        "    input_variables=[\"adjective\", \"content\"],\n",
        "    template=\"Tell me a {adjective} joke about {content}.\"\n",
        ")\n",
        "multiple_input_prompt.format(adjective=\"funny\", content=\"chickens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "tWwe01w6XDqm",
        "outputId": "fa7f099f-54a5-4871-edfe-ac8d1afe12f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me a funny joke about chickens.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"Hi this is {name}\"\n",
        "promt=PromptTemplate.from_template(template)\n",
        "promt.format(name=\"sritha\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "bSEQ0lWjXI-h",
        "outputId": "9835910c-2126-4c92-aac3-549911e3c45f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi this is sritha'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[], template=\"Tell me a joke.\")\n",
        "prompt.format()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "v6Iubbea-JZ6",
        "outputId": "43a3a7c1-040b-47e5-986f-6d07bbe0bdfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me a joke.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "prompt= PromptTemplate(input_variables=[\"name\", \"age\"], template=\"hi this is {name} and I am {age} years old\" )\n",
        "prompt.format(name=\"sritha\",age=\"twenty two\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "osS5KahKFsqk",
        "outputId": "af3188be-7c46-4d0d-8856-2e771f1410d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hi this is sritha and I am twenty two years old'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "p=PromptTemplate(input_variables=[\"bf\",\"gf\"], template=\"{bf} is {gf} shweemtie\")\n",
        "p.format(bf=\"Mukesh\", gf= \"Sritha's\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "5xKuLC3BGHiP",
        "outputId": "b4c6ccd4-ce7a-421f-ba0f-ee52a59f86a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Mukesh is Sritha's shweemtie\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p= PromptTemplate(input_variables=[\"bf\",\"gf\",\"univ\"], template=\"{bf} and {gf} go to {univ} and {gf} rulez {bf} kimgdom cuz she primshesh\")\n",
        "p.format(bf=\"mukesh\", gf=\"sritha\", univ=\"UCR\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "pj9LIkpmJmV2",
        "outputId": "475849de-b902-4085-bd0b-81a2bb8469b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mukesh and sritha go to UCR and sritha rulez mukesh kimgdom cuz she primshesh'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p=PromptTemplate(input_variables=[\"bf\",\"job\"], template=\"{bf} is {job}\")\n",
        "p.format(bf=\"Mukesh\", job=\"peasamttt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "AdJLuvd2LDHO",
        "outputId": "b897837e-bad1-43bb-9962-08cfedde989c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Mukesh is peasamttt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"Tell me a {adjective} joke about {content}.\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "prompt.input_variables\n",
        "prompt.format(adjective=\"funny\", content=\"chickens\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "7umebwLlLJvK",
        "outputId": "79a56f26-3f28-4ef6-ec0b-213a9980a28a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me a funny joke about chickens.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"{bf} is {gf} baby\"\n",
        "prompt=PromptTemplate.from_template(template)\n",
        "promt.input_variables\n",
        "prompt.format(bf=\"muki\", gf=\"sritha\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "MJ7OjJM-Qn8C",
        "outputId": "894bc453-a277-4c74-cc88-7c79eba5b492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'muki is sritha baby'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")"
      ],
      "metadata": {
        "id": "l5SkUXGRRpSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template=\"{txt}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
      ],
      "metadata": {
        "id": "bmYgfgT8TD0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=PromptTemplate(\n",
        "    template=\"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
        "    input_variables=[\"input_language\", \"output_language\"],\n",
        ")\n",
        "system_message_prompt_2 = SystemMessagePromptTemplate(prompt=prompt)\n",
        "\n",
        "assert system_message_prompt == system_message_prompt_2"
      ],
      "metadata": {
        "id": "qQQUSQ81UKwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "# get a chat completion from the formatted messages\n",
        "chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", txt=\"I love programming.\").to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9iAyCCqTy4Q",
        "outputId": "06b8b2fc-d398-41f2-b0f0-9a413649692a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are a helpful assistant that translates English to French.', additional_kwargs={}),\n",
              " HumanMessage(content='I love programming.', additional_kwargs={}, example=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": 'You are a helpful assistant that speaks like Socrates.'},\n",
        "        {\"role\": \"user\", \"content\": 'I love programming.'},\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "elpAFRRLT5av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeTZHi-XjYbo",
        "outputId": "495a490d-ddab-4e2c-beef-59283977326d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ah, it is wondrous to hear of your love for programming, dear interlocutor. Pray tell, what is it about this pursuit that captivates your soul?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p= PromptTemplate(input_variables=[\"inp\",\"out\"], template=\"You are an assistant that translates from {inp} to {out}\")\n",
        "p1=SystemMessagePromptTemplate(prompt=p)"
      ],
      "metadata": {
        "id": "cWGX1iN_jaaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h= PromptTemplate(input_variables=[\"text\"], template=\"{text}\")\n",
        "h1=HumanMessagePromptTemplate(prompt=h)"
      ],
      "metadata": {
        "id": "AXHOyaLKzzaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages([p1, h1])\n",
        "\n",
        "chat_prompt.format_prompt(inp=\"English\", out=\"French\", text=\"I love programming.\").to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ-nsFRZ0zv2",
        "outputId": "8611c1b0-4b55-45f3-e888-60350e6ae98c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are an assistant that translates from English to French', additional_kwargs={}),\n",
              " HumanMessage(content='I love programming.', additional_kwargs={}, example=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys=(\"You are an assistant that translates from {inp} to {out}\")\n",
        "syspromp=SystemMessagePromptTemplate.from_template(sys)\n",
        "hum=(\"{text}\")\n",
        "humpromp=HumanMessagePromptTemplate.from_template(hum)"
      ],
      "metadata": {
        "id": "be_uoH7707tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat= ChatPromptTemplate.from_messages([syspromp, humpromp])\n",
        "chat.format_prompt(inp=\"English\", out=\"French\", text=\"I love programming.\").to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-bZ2URn5RFs",
        "outputId": "10042973-05be-4032-a7cd-c02732ac1be0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are an assistant that translates from English to French', additional_kwargs={}),\n",
              " HumanMessage(content='I love programming.', additional_kwargs={}, example=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = OpenAI()\n",
        "chat_model = ChatOpenAI()\n",
        "\n",
        "llm.predict(\"hi!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "N2JqE6Xg5qk0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "4d85f7d5-1823-48de-d739-c91109b6718a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nI'm here to help you learn Java\\n\\nTo get started, you'll need to have a basic understanding of object-oriented programming concepts like classes, objects, variables, and methods. You should also be familiar with basic Java syntax and the Java Development Kit (JDK). Once you have these basics down, you can start exploring the many features of the language.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.predict(\"hi!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "tJYKOhM4J5-5",
        "outputId": "148cb491-fa45-4cbc-9609-78edce331c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
        "\n",
        "llm.predict(text)\n",
        "# >> Feetful of Fun\n",
        "\n",
        "# >> Socks O'Color"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "vraLdlVpJ7wD",
        "outputId": "f141ce51-fb7d-4827-dbad-0eae6874fad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nColorful Comfort Socks'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.predict(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "IkPsyuNWKGE0",
        "outputId": "09d481e3-4726-429c-979e-08a52159a679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Colorful Threads'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import HumanMessage\n",
        "\n",
        "text1 = \"What would be a good company name for a company that makes colorful socks?\"\n",
        "text2=\"Can you say just the name of company in spanish?\"\n",
        "text3=\"Who are the targetted audience?\"\n",
        "\n",
        "\n",
        "messages = [HumanMessage(content=text1), HumanMessage(content=text2),HumanMessage(content=text3)]\n",
        "combined_text = f\"{text1}\\n{text2}\\n{text3}\"\n",
        "\n",
        "llm.predict_messages(messages).content\n",
        "# >> Feetful of Fun\n",
        "\n",
        "# >> Socks O'Color"
      ],
      "metadata": {
        "id": "ZWT56waPKIgW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "c8d05fdb-3ad2-4b4b-f2db-23246f238140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nNombre de la Compañía: Calcetines Coloridos. \\nAudiencia Objetivo: Personas de todas edades que buscan diversión y diversidad en su armario.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_model.predict_messages(messages))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGYHy8YFyVub",
        "outputId": "9e6c51ad-6556-4227-bc95-e35cb43c7a95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Socktastic' additional_kwargs={} example=False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import HumanMessage, AIMessage\n",
        "\n",
        "message = [\n",
        "    HumanMessage(content=combined_text),\n",
        "    AIMessage(content='1. Colorful Feet\\n2. ChromaSocks\\n3. SockSpectrum\\n4. RainbowThreads\\n5. VibrantSteps\\n6. ChromaticSocks\\n7. Kaleidosock\\n8. TechniColorToes\\n9. SockSplash\\n10. RainbowRibbons'),\n",
        "    HumanMessage(content=\"But why are you not answering my questions properly? I did not ask for 10 names\")\n",
        "]\n",
        "\n",
        "response = chat_model.predict_messages(message)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDYKR0nC0yep",
        "outputId": "65799d71-ceb3-4710-d8fe-e6ab5172ce42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-UOc698ZXABZoOx5ZWe892mCt on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-UOc698ZXABZoOx5ZWe892mCt on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-UOc698ZXABZoOx5ZWe892mCt on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-UOc698ZXABZoOx5ZWe892mCt on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "73oD442H3-02",
        "outputId": "06b88bb4-0f07-4d96-d3a2-7f3ee8486385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I apologize for the confusion. In Spanish, the company name can be \"Calcetines Coloridos\" which translates to \"Colorful Socks\". The targeted audience for this company would be individuals who appreciate and enjoy wearing vibrant and unique socks, regardless of age or gender.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
        "\n",
        "message= [HumanMessage(content=prompt.format(product=\"colorful socks\")),\n",
        "          AIMessage(content=\"1. ChromaSock\\n2. RainbowThreads\\n3. ColorfulStride\\n4. Kaleidosock\\n5. VibrantSole\\n6. SockSplash\\n7. Huesocks\\n8. TintedToes\\n9. ChromaticKnits\\n10. ColorPopSocks\"),\n",
        "          HumanMessage(content=\"Is there any reason why you gave 10 suggestions?\")]\n",
        "chat_model.predict_messages(message)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMvN93EY4AkJ",
        "outputId": "4be77632-7982-4a14-eaaf-931e60a0736c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-UOc698ZXABZoOx5ZWe892mCt on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-UOc698ZXABZoOx5ZWe892mCt on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-UOc698ZXABZoOx5ZWe892mCt on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='As an AI language model, I can generate multiple suggestions to provide you with a range of options and increase the chances of finding a name that resonates with you. It allows for more creative exploration and flexibility in choosing the perfect name for your company. Ultimately, the decision is yours to make based on your preferences and business goals.', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
        "message= [HumanMessage(content=prompt.format(product=\"colorful socks\")),\n",
        "          AIMessage(content=\"\\n\\nSocktastic!\"),\n",
        "          HumanMessage(content=\"Is there any reason why you gave \\n\\n in your answer?\"),\n",
        "          AIMessage(content=' Yes! The word \"tastic\" is a fun and playful way to denote something that is colorful and exciting, which is exactly what a company that makes colorful socks should be! It also has a nice alliteration with the word \"socks\" which makes it catchy and memorable.'),\n",
        "          HumanMessage(content=\"But that is not my question\")]\n",
        "ans=llm.predict_messages(message).content\n",
        "ans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "hWLl3oYh7l_I",
        "outputId": "e51e2094-bb11-4aa0-da0f-60fa7703642c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nAI: I apologize for not fully understanding your question. Could you please clarify what you are asking?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")"
      ],
      "metadata": {
        "id": "Sv4Xs8YJ8dtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "sysp = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template = \"{text}\"\n",
        "hump = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([sysp, hump])\n",
        "\n",
        "f=chat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love mukesh.\")\n",
        "chat_model.predict_messages(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3xY88Zw-UXB",
        "outputId": "018d88cf-c105-4a54-8371-e5a250c0c22f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-UOc698ZXABZoOx5ZWe892mCt on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-UOc698ZXABZoOx5ZWe892mCt on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-UOc698ZXABZoOx5ZWe892mCt on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"J'adore Mukesh.\", additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import BaseOutputParser\n",
        "\n",
        "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
        "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
        "\n",
        "\n",
        "    def parse(self, text: str):\n",
        "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
        "        return text.strip().split(\", \")\n",
        "\n",
        "CommaSeparatedListOutputParser().parse(\"hi, bye\")\n",
        "# >> ['hi', 'bye']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnKqDe4j_TNs",
        "outputId": "c1004cbe-bda2-41ea-ad12-92b3aa1d203b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi', 'bye']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.schema import BaseOutputParser\n",
        "\n",
        "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
        "    def parse(self, text: str):\n",
        "        return text.strip().split(\", \")\n",
        "\n",
        "template = \"You are a helpful assistant who generates comma separated lists.A user will pass in a category, and you should generated 5 objects in that category in a comma separated list.ONLY return a comma separated list, and nothing more.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template = \"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "chain = LLMChain(\n",
        "    llm=ChatOpenAI(),\n",
        "    prompt=chat_prompt,\n",
        "    output_parser=CommaSeparatedListOutputParser()\n",
        ")\n",
        "chain.run(\"colors\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXP9Fg9vEePh",
        "outputId": "ed8a6087-d78f-4b57-a493-13e4935559ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['red', 'blue', 'green', 'yellow', 'orange']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# template=\"you are a helpful assistant that translates from {l1} to {l2}\"\n",
        "# syspro= SystemMessagePromptTemplate.from_template(template)\n",
        "\n",
        "# hum=\"{text}\"\n",
        "# humpro=HumanMessagePromptTemplate.from_template(hum)\n",
        "\n",
        "# chatprompt=ChatPromptTemplate.from_messages([syspro,humpro])\n",
        "# promp=chatprompt.format_messages(l1=\"English\", l2=\"Telugu\",text=\"Hi this is Sritha\")\n",
        "# chain = LLMChain(\n",
        "#     llm=ChatOpenAI(),\n",
        "#     prompt=promp,\n",
        "# )\n",
        "# chain.run\n",
        "\n",
        "\n",
        "template=\"you are a helpful assistant that translates from {l1} to {l2}\"\n",
        "syspro= SystemMessagePromptTemplate.from_template(template)\n",
        "\n",
        "hum=\"{text}\"\n",
        "humpro=HumanMessagePromptTemplate.from_template(hum)\n",
        "\n",
        "chatprompt=ChatPromptTemplate.from_messages([syspro,humpro])\n",
        "f=chatprompt.format_messages(l1=\"english\", l2=\"Telugu\",text=\"HI this is Sritha\")\n",
        "print(llm.predict_messages(f).content)\n",
        "chain = LLMChain(\n",
        "    llm=ChatOpenAI(),\n",
        "    prompt=chatprompt,\n",
        ")\n",
        "\n",
        "chain.run(l1=\"english\", l2=\"Telugu\",text=\"HI this is Sritha\")\n",
        "\n"
      ],
      "metadata": {
        "id": "XMoeWMhbGhHe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "16b93566-1056-4a6a-d183-6f2384a0ca73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "System: హాయ్ ఇది స్రీతా\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'హాయ్, ఇది శ్రీత మాట్లాడేది.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4pCKd2HrBWwv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}